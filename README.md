# Jupyter notebooks for "Ensemble Methods for Machine Learning"

This repository contains companion material: data, Python code and Jupyter notebooks for [Ensemble Methods for Machine Learning (Manning Publications)](https://www.manning.com/). The code and notebooks are released under the [MIT license](https://github.com/gkunapuli/ensemble-methods-notebooks/blob/master/LICENSE).

These notebooks primarily use Python 3.7, scikit-learn 0.21 and matplotlib 3.2.1, though other packages such as pandas, seaborn and Keras make guest appearances as well.

This book is a work in progress and expected to be released in late 2020.

## List of notebooks

* Chapter 1. Ensemble Methods: Hype or Halleujah?
    * [1.3. Fit vs. Complexity in Machine-Learning Models](https://nbviewer.jupyter.org/github/gkunapuli/ensemble-methods-notebooks/blob/master/Ch1.3-fit-vs-complexity.ipynb?flush_cache=true)
    * [1.4. A Simple Model Averaging Ensemble](https://nbviewer.jupyter.org/github/gkunapuli/ensemble-methods-notebooks/blob/master/Ch1.4-model-averaging-example.ipynb?flush_cache=True)
    
* Chapter 2. Homogeneous Parallel Ensembles: Bagging and Random Forests
   * [2.2, 2.3. Bagging and Random Forest](https://nbviewer.jupyter.org/github/gkunapuli/ensemble-methods-notebooks/blob/master/Ch2.2and2.3-bagging-and-random-forest.ipynb?flush_cache=True)
   * [2.5. Case Study: Breast Cancer Diagnosis](https://nbviewer.jupyter.org/github/gkunapuli/ensemble-methods-notebooks/blob/master/Ch2.5-case-study-breast-cancer-diagnosis.ipynb?flush_cache=True)

* Chapter 3: Heterogeneous Parallel Ensembles: Combining Strong Learners

* Chapter 4: Sequential Ensembles: Boosting

* Chapter 5: Sequential Ensembles: Gradient Boosting

* Chapter 6: Sequential Ensembles: Newton Boosting
